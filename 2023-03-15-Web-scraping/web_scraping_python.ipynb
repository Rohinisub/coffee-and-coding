{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a662bc0-abdf-4172-a7f2-4605f65ec508",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Web scraping in Python: Scraping with Requests and BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cfe14-c38e-40af-86bc-855e4d2ffc72",
   "metadata": {},
   "source": [
    "Web scraping is a valuable tool for programmers to effortlessly gather information from the vast resources of the internet. While it is generally acceptable for non-commercial purposes with publicly available data, caution should be taken to avoid scraping protected information such as personal data, intellectual property, or confidential information. \n",
    "\n",
    "Additionally, the complexities of scraping social media due to its varying levels of accessibility highlight the need for cautious and informed scraping practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a46aca-f5a1-40e0-96eb-2b8efa2c96bc",
   "metadata": {},
   "source": [
    "This coffee and coding session, we will focus to use Python's two libraries; <b>Requests</b> and <b>BeautifulSoup</b>. <br>http://books.toscrape.com/ contains review  for fake books for the beginners learning web scrapings. <br>\n",
    "This session aims for the beginners (like me!) introduction to web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab24ca1-55a9-49f6-b971-bfa78537b9d4",
   "metadata": {},
   "source": [
    "To gather information from the internet through web scraping, one typically follows a four-step process:\n",
    "\n",
    "<li> Sending an HTTP GET request to the URL </li>\n",
    "<li> Retrieving HTML (Hypertext Markup Language) content </li>\n",
    "<li> Building the HTML document tree </li>\n",
    "<li> Extracting information from the HTML document tree </li>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bf4eb-093a-4405-bd27-dd4766f85acc",
   "metadata": {},
   "source": [
    "### Requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca3f7d-1090-42e3-a151-98a11e8341f1",
   "metadata": {},
   "source": [
    "The Requests library in Python is a popular and widely used library for making HTTP requests. This library allows user to send HTTP requests to server, receive response and handle in a simple and efficient manner. \n",
    "\n",
    "It supports varous methods for making requests, such as GET, POST, HEAD, PUT, DELETE etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70249c0b-f079-478b-9367-702c130421a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04815663-deda-4065-9faa-a43d15bd369b",
   "metadata": {},
   "source": [
    "BeautifulSoup is a Python library for web scraping and data extraction from HTML and XML files. <br>\n",
    "It provides a convenient and efficient way to parse and naviage through HTML contents, allowing for the extraction of specific elements and data.\n",
    "\n",
    "<b>Resource: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2ceae-1aad-4a89-9091-39b8bd920c3c",
   "metadata": {},
   "source": [
    "We will first fetch HTML code from our fake-books site. But before we start, I will show you a very quick overview of basic HTML. \n",
    "\n",
    "HTML is a standard markup language used for creating web pages and other information that can be displayed in a web browser. It consists of a set of tags and attributes that define the structure, content, and appearance of a web page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a59da51-95b8-46bf-a6d1-894bcbfc2e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71072523-70e0-4626-b747-28a9c70f3225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "html_test = \"\"\"\n",
    "<html>\n",
    "  <head>\n",
    "    <title>My First Web Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Welcome to my first test web page</h1>\n",
    "    <p>This is a paragraph of text. I want to list some of my favourite composers and their music \n",
    "        <i> Italic font </i>    \n",
    "        <b> Bold text 1</b>\n",
    "        <b> Bold text 2</b>\n",
    "    </p>    \n",
    "    <p> <b color = \"blue\"> This is next paragraph </b></p>\n",
    "    <br>    \n",
    "    <ul id = \"composer\" class = \"myclass\">\n",
    "      <li>Liszt</li>  \n",
    "      <a href = \"https://en.wikipedia.org/wiki/Franz_Liszt\"> Franz Liszt: Hungarian composer, pianist of romantic period. </a>\n",
    "      <li>Mozart</li>\n",
    "      <a href = \"https://en.wikipedia.org/wiki/Wolfgang_Amadeus_Mozart\"> Wolfgang Amadeus Mozart influential composer of classical period. </a>\n",
    "      <li>Debussy</li>\n",
    "      <a href = \"https://en.wikipedia.org/wiki/Claude_Debussy\"> Claude Debussy, French composer seen as the first impressionist. </a>\n",
    "      <li>Beethoven</li>\n",
    "      <a href = \"https://en.wikipedia.org/wiki/Ludwig_van_Beethoven\"> Ludwig van Beethoven, German composer and pianist.</a>\n",
    "    </ul>\n",
    "    <ul id = \"piece\" class = \"myclass\">\n",
    "        <li>Love Dream (Liebestraum) (No.3)</li>\n",
    "        <li>Piano Sonata No.16</li>\n",
    "        <li>Moonlight (Clair de lune)</li>\n",
    "        <li>Symphony No.5</li>\n",
    "    </ul>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85c1805d",
   "metadata": {},
   "source": [
    "The `<ul>` HTML element represents an unordered list of items, typically rendered as a bulleted list\n",
    "\n",
    "The `<li>` tag defines a list item.\n",
    "\n",
    "The HTML class attribute is used to specify a class for an HTML element. Multiple HTML elements can share the same class. (https://www.w3schools.com/html/html_classes.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95168506-313a-4ac1-9e2b-0ce8ec18a146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's import libraries first \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b0a05f-f129-4a0c-86f7-662f44608426",
   "metadata": {},
   "source": [
    "We explore functions in BeautifulSoup using the sample HTML we've just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24ee1f-bdbb-4a1f-9780-f749affe6426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create soup variable\n",
    "soup = BeautifulSoup(html_test, features = \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68a05a-e3f2-44b7-a94f-a6a5bfe08073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# soup has the information extracted from HTML string. We can make it better display\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c8e9f-9284-4ed6-b1f2-b2fc6d0386ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now indent etc works better\n",
    "# Prettify arranges all the tags in a parse-tree manner with better readability.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2d12d4-2977-4f89-89d2-f07a910de42a",
   "metadata": {},
   "source": [
    "When you read HTML, you want to search specific aspect. You can find thins by `tag`. like `<head>`,`<title>` etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea8408f-2ce7-4274-9004-7da989c581de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Codes to nativage data structure (https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "display(soup.title) # access title tag\n",
    "display(soup.title.string) # acces title tag and want only access what is inside in the tag\n",
    "# You can also modify string in your tag\n",
    "# soup.title.string = \"My Second web page\"\n",
    "display(soup.p.get_text())\n",
    "display(soup.ul.get_text()) # it returns first ul element only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916076ca-01e3-42b2-a77b-4df6af4b1d9c",
   "metadata": {},
   "source": [
    "Let's try `find()` or `find_all()` functions to search for specific tags in the HTML content. <br>\n",
    "`find()` returns only the first occurrence of the search query. `find_all()` returns a list of all matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eaeac8-6704-452e-8a07-9a94f6cc9a38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(soup.find('a')) # first element of a tag only\n",
    "display(soup.find('a').text) # This allows us to extract the inner HTML text\n",
    "display(soup.find_all('a')) # all a tag elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98dedff-4ffc-4ffd-b2a0-42f988d26874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(soup.find_all(\"p\")[0]) # First element of p tag\n",
    "display(soup.find_all(\"p\")[0].find_all(\"b\")) # inside p tag all b tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd866e-1652-4de2-ad8a-5e834026e27c",
   "metadata": {},
   "source": [
    "If we only want to extract composer items `<li>`, we can use `attr ={}` dictionary to define the attributes of an HTML tag. Dictionary keys are the name of the attributes, and the values are the attribute values.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72379edf-61c2-41f5-9712-a03b689b2032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myList = soup.find(attrs = {'id':'composer', 'class':'myclass'})\n",
    "myList.find_all('li')\n",
    "\n",
    "# if you want to extract everything from the myclass attribute:\n",
    "# soup.find_all(attrs = {'class':'myclass'}) # in this case, it will return all names of composers and their music pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e975970f-092a-4011-8024-a213a58ef536",
   "metadata": {},
   "source": [
    "You can also traverse the parent and children elements in the HTML code. Below code uses `find_parent()` function to find the parent of the first `ul` tag (`body` tag). It then uses `find_all()` function again to find all the `li` elements in the first `ul` tag and print each `li` element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff8914-2a40-4461-a3ed-5365431f81d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First find the first ul tag\n",
    "first_ul = soup.find(\"ul\")\n",
    "\n",
    "# Find the parent of the first ul tag (the body tag)\n",
    "body = first_ul.find_parent(\"body\")\n",
    "\n",
    "# Print the text content of each li element\n",
    "for li in first_ul.find_all([\"a\", \"li\"]):\n",
    "    print(li.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a7627-5f97-4961-836f-a0191e6a6eb5",
   "metadata": {},
   "source": [
    "### Let's try with fake-book review site "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83460a-fb28-41ca-af94-c338cc745b13",
   "metadata": {},
   "source": [
    "We've tried few basics of beautifulsoup functions using our own HTML text. Now, we can go to fake book review sites and extract some information about the books, their prices and book description.\n",
    "Before we start, let's check this webpage and inspect HTML codes. https://books.toscrape.com/catalogue/page-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467d0de-63b2-4051-aec1-57dc4debe77e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import requests and pandas, re as well for regex string manipulation \n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99e0f18-5da3-4550-8ad3-50655949fa71",
   "metadata": {},
   "source": [
    "#### Read the first page, first book information\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d40ef-d569-4806-934a-b7d015d42967",
   "metadata": {},
   "source": [
    "First, read book title, price and rating. We will then extend this by adding book description. (When user click book title, it takes to the product description page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d9e50-afde-4fc2-9599-eabc81dd2136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# request can get HTML data from the very first page.\n",
    "url = \"https://books.toscrape.com/catalogue/page-1.html\" #page-2 etc will repeat to extract all pages. which we will try in the next section\n",
    "response = requests.get(url) # http request and get the content of the page\n",
    "display(response)\n",
    "# 200 means 'Success' status\n",
    "# If you want to see content of resonse (it will be messy)\n",
    "# response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73359e24-c27c-46a8-940d-5c4d00e40ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "# You can look at the content using \n",
    "# soup.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351487b6-7d24-411f-9a94-c6a713dc32d4",
   "metadata": {},
   "source": [
    "Let's go back to our html inspect page extract our interest (book title, price) from the first page."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfeffd9c-4582-409c-b3bd-bb8aeae2e0fa",
   "metadata": {},
   "source": [
    "Below code will extract book title, its price and rating. <br>\n",
    "First, we will find the first `<article>` tag. From there, we can search `title` attribute from `<h3>/<a>` tag. It will return the book title.\n",
    "Next, we will search and find `<p>` tag from the child of `<article>` tag and look for `price_color` attribute and return its text (£54.12 for example).\n",
    "We only want to keep price (float) number, so cleaning that field by using regex grouping.\n",
    "Similarly, we extract `<p>` tag `<class>` attribute second element (One, Two, Three etc) and convert it into integer value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919328f-bf2a-4790-9dea-f2ed11273e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract first book in the first page to see if this is working.\n",
    "review_mapping = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5} # For mapping for numbers\n",
    "\n",
    "article = soup.find(\"article\")\n",
    "title = article.h3.a[\"title\"]\n",
    "print(title)\n",
    "price = article.find('p', class_=\"price_color\").text \n",
    "price = float(re.findall(\"\\d+\\.\\d+\", price)[0]) #regex + is metacharacters means one or more occurrences\n",
    "print(price)\n",
    "rating = article.p[\"class\"][1] # access secound element one, two, three etc \n",
    "rating = review_mapping[rating]\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527d445-948f-4d88-94d8-7a525795be06",
   "metadata": {},
   "source": [
    "Expand it for all page 1 using for loop. As you can see we now use `find_all()` function to find all `<article>` tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b095b20-a65a-4159-884f-c43defdbe303",
   "metadata": {},
   "source": [
    "#### Read the first page, all books in the first page information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22fd8c-6074-4108-bc19-e6ea3555a1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Based on the code above, we can now extract all books in the first page.\n",
    "books = [] # We will append to this list, define empty list\n",
    "\n",
    "for article in soup.find_all(\"article\"):\n",
    "    title = article.h3.a[\"title\"]\n",
    "    # price = article.select_one(\".price_color\").get_text() # another way using select_one (https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors)    \n",
    "    price = article.find('p', class_=\"price_color\").text \n",
    "    price = float(re.findall(\"\\d+\\.\\d+\", price)[0])\n",
    "    rating = article.p[\"class\"][1] # access secound element one, two etc \n",
    "    rating = review_mapping[rating]\n",
    "    books.append({\"title\": title, \"price\": price, \"rating\":rating})\n",
    "      \n",
    "for book in books[0:5]: # First 5 books\n",
    "    print(f\"Title: {book['title']}\")\n",
    "    print(f\"Price: {book['price']}\")\n",
    "    print(f\"Rating: {book['rating']}\")\n",
    "    print(\"\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e1fcd2-f566-450e-b6da-2b38de26967e",
   "metadata": {},
   "source": [
    "So far, we extracted book title, price, rating. Can we also add extra information? I would like to add product description, topic (Travel/Poetry/Mystery etc etc). To achieve this, we can extract hyperlink for each book and extract description, genre from individual pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa68ddf-9a01-4bd3-b0cc-d913b10bebb7",
   "metadata": {},
   "source": [
    "#### Read the first page, all books in the first page information and extract extra information from individual books (description, genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6944f-8b76-47b1-bbed-01d42fe9e5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the code from above, include book description and genre\n",
    "# We need to get hyperlink and get information from each book page\n",
    "# Extract book description and genre from the page.\n",
    "\n",
    "# request the main page\n",
    "url = \"https://books.toscrape.com/catalogue/page-1.html\" #page-2 etc will repeat to extract all pages for next step. \n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# # Find all book titles, prices, and customer reviews\n",
    "books = []\n",
    "review_mapping = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5} # For mapping for numbers \n",
    "\n",
    "# This code is expanding from the previous codes.\n",
    "\n",
    "\n",
    "for article in soup.find_all(\"article\"):\n",
    "    title = article.h3.a[\"title\"]\n",
    "    price = article.find('p', class_=\"price_color\").text \n",
    "    price = float(re.findall(\"\\d+\\.\\d+\", price)[0]) # Extract numbers (price) only. Remove special character & £ sign.\n",
    "    rating = article.p[\"class\"][1] # access secound element one, two etc \n",
    "    rating = review_mapping[rating]\n",
    "    \n",
    "    # find the link to the individual book's page \n",
    "    link = article.h3.a[\"href\"]       \n",
    "    book_url = \"http://books.toscrape.com/catalogue/\" + link  # get information of individual book link page to take you to the description  \n",
    "    \n",
    "    # Again request the individual book's page    \n",
    "    book_response = requests.get(book_url)    \n",
    "    book_soup = BeautifulSoup(book_response.text, \"html.parser\")\n",
    "    \n",
    "    # extract the product description\n",
    "    # select a meta tag with attribute name = \"description\". The method select_one returns the first matching element, or 'None'\n",
    "    # if there are not matches. ['content'] part is accessing the value of the content attribute of the selected meta tag. \n",
    "    \n",
    "    product_description = book_soup.select_one(\"meta[name='description']\")[\"content\"]  \n",
    "    genre_related_tag = book_soup.select(\"ul.breadcrumb li\") # all li elements\n",
    "    # print(genre_related_tag)\n",
    "    genre_list = [item.text for item in genre_related_tag]\n",
    "    # print(genre_list) \n",
    "    # it will extract text from tags and this website follows this structure\n",
    "    # home/books/{topic}/title\n",
    "    # we will extract [-2] second-to-last item (poetry, historical fiction, fiction etc)\n",
    "    genre = genre_list[-2] if len(genre_list) > 2 else None # control error, extract only if list is more than 2 (otherwise it will throw error)    \n",
    "    books.append({\"title\": title, \"genre\": genre, \"price\": price, \"rating\":rating, \"book_description\": product_description})\n",
    "    \n",
    "for book in books[0:5]: # First 5 books\n",
    "    print(f\"Title: {book['title']}\")\n",
    "    print(f\"Genre: {book['genre']}\")    \n",
    "    print(f\"Price: {book['price']}\")\n",
    "    print(f\"Rating: {book['rating']}\")\n",
    "    print(f\"Description: {book['book_description']}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5d39f5-d634-4689-98af-99e696c57a86",
   "metadata": {},
   "source": [
    "We have now include information such as:\n",
    "<li> Book title </li>\n",
    "<li> Genre </li>\n",
    "<li> Price </li>\n",
    "<li> Rating </li>\n",
    "<li> Book description </li>\n",
    "\n",
    "It would be useful if we can go through all pages and extract all books they have in this website. This is our final stage and we will save this as `pandas DataFrame`. \n",
    "\n",
    "How do we get the total number of pages? We will find `<li>` tag with `next` class first. Based on this we search previous sibling `<li>` tag using `find_previous_sibling('li')`. \n",
    "\n",
    "[0] requires to access first element as `find_previous_sibling` can only access single BeautifulSoup object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75625bc-f1c3-4b85-ab12-5bc4fbfa293a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Expand for all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931ab0f-51b0-4c7c-81ba-5aaf405c6b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review_mapping = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5}\n",
    "books = []\n",
    "\n",
    "# Get the total number of pages\n",
    "url = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# We use this code to extract the total pages (how many loop we need to run to go through all pages)\n",
    "previous_page = soup.select(\"li.next\")[0].find_previous_sibling(\"li\")\n",
    "if previous_page:\n",
    "    text = previous_page.text # which returns page 1 of 50 and we want to extract 50 from here.\n",
    "    print(text)\n",
    "    # in here extract the last page number \n",
    "    match = re.search(r'Page (\\d+) of (\\d+)', text) # any number\n",
    "    total_pages = int(match.group(2)) # extract last page.\n",
    "    print(total_pages)\n",
    "else:\n",
    "    total_pages = 1 # prevent index out of range error\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each page\n",
    "\n",
    "# for page_number in tqdm(range(1, total_pages + 1)):\n",
    "# note f: “formatted string literals,” f-strings are string literals that have an f at the beginning and curly braces containing expressions that will be replaced with their values.\n",
    "#     url = f\"http://books.toscrape.com/catalogue/page-{page_number}.html\"\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#     for article in soup.find_all(\"article\"):\n",
    "#         title = article.h3.a[\"title\"]\n",
    "#         price = article.find('p', class_=\"price_color\").text \n",
    "#         price = float(re.findall(\"\\d+\\.\\d+\", price)[0]) # Extract numbers (price) only. Remove special character & £ sign.\n",
    "#         rating = article.p[\"class\"][1] # access secound element one, two etc \n",
    "#         rating = review_mapping[rating]\n",
    "\n",
    "#         # find the link to the individual book's page \n",
    "#         link = article.h3.a[\"href\"]       \n",
    "#         book_url = \"http://books.toscrape.com/catalogue/\" + link  # get information of individual book link page to take you to the description  \n",
    "\n",
    "#         # Again request the individual book's page    \n",
    "#         book_response = requests.get(book_url)    \n",
    "#         book_soup = BeautifulSoup(book_response.text, \"html.parser\")\n",
    "\n",
    "#         # extract the product description\n",
    "#         # select a meta tag with attribute name = \"description\". The method select_one returns the first matching element, or 'None'\n",
    "#         # if there are not matches. ['content'] part is accessing the value of the content attribute of the selected meta tag. \n",
    "\n",
    "#         product_description = book_soup.select_one(\"meta[name='description']\")[\"content\"]  \n",
    "#         genre_related_tag = book_soup.select(\"ul.breadcrumb li\") # all li elements\n",
    "#         genre_list = [item.text for item in genre_related_tag]\n",
    "#         # print(genre_list) \n",
    "#         # it will extract text from tags and this website follows this structure\n",
    "#         # home/books/{topic}/title\n",
    "#         # we will extract [-2] second-to-last item (poetry, historical fiction, fiction etc)\n",
    "#         genre = genre_list[-2] if len(genre_list) > 2 else None # control error, extract only if list is more than 2 (otherwise it will throw error)    \n",
    "#         books.append({\"title\": title, \"genre\": genre, \"price\": price, \"rating\":rating, \"book_description\": product_description}) \n",
    "\n",
    "        \n",
    "        \n",
    "# # # Convert the list of dictionaries to a Pandas dataframe\n",
    "# # # We will clean the text\n",
    "# df = pd.DataFrame(books)\n",
    "\n",
    "# df.head()\n",
    "# # We will use this for the next.\n",
    "# df.to_csv(\"books_web_scraping.csv\", index = False)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947d392-e5eb-4bdb-9bb5-c74a99cd0cf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pandas manipulation \n",
    "\n",
    "<li> Tidy up book description text by removing non alphanumeric characters. </li>\n",
    "<li> Descriptive analysis </li>    \n",
    "<li> We could try NLP on book_description? </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c4099-b21a-4742-b528-051cf931d68a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We don't want to run it during the session (takes 7mins) so we will read as csv file.\n",
    "url = 'https://raw.githubusercontent.com/nhsbsa-data-analytics/coffee-and-coding/master/2023-03-15-Web-scraping/books_web_scraping.csv'\n",
    "df = pd.read_csv(url, index_col = 0 )\n",
    "df.head() # genre, book_description could do with cleaning\n",
    "\n",
    "df['genre'] = df['genre'].astype(str)\n",
    "df['book_description'] = df['book_description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fb7a0-97d5-4a3b-b256-43c11fa394ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# substitute any non alphanumeric character to empty string\n",
    "# Also notice that all review end with ...more\n",
    "df[['genre','book_description']] = df[['genre','book_description']].applymap(lambda x: re.sub(r'\\n|[^\\w\\s\\']+',' ',x))\n",
    "df[['genre','book_description']] = df[['genre','book_description']].applymap(lambda x: x.strip())\n",
    "df[['book_description']] = df[['book_description']].applymap(lambda x: re.sub(r'...more','',x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63b852-ffc1-4b5e-808c-b6b5b6504a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82077842-dd43-4078-b3fd-d15368d2bb3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_df = df['genre'].value_counts() \\\n",
    "           .rename('count') \\\n",
    "           .reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbde59f7-1118-449f-8dab-bc6b1297c740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1348168-f696-425f-b127-fd2079f3b1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['book_description'][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc56dec-93a9-477a-978d-f51c2a32131f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings = df.groupby('rating')['rating'].count() \\\n",
    "            .rename('count') \\\n",
    "            .reset_index()\n",
    "\n",
    "print(ratings)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.bar(ratings['rating'], ratings['count'])\n",
    "\n",
    "# add labels and title\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of Ratings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec45e9-dded-4641-899f-293756e73704",
   "metadata": {},
   "source": [
    "### As we have book description, we could apply NLP technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b545851-b77f-40a3-bee5-45120af47687",
   "metadata": {},
   "source": [
    "We can try to extract keywords from the text using `nltk` and `RAKE` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46ac096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use google colab, you will need to run this code (uncomment below code and run this cell)\n",
    "# !pip install python-rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f20ed-26fe-43f8-a95d-6e429eb17b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import RAKE\n",
    "import re\n",
    "import nltk # Natural Language Toolkit library\n",
    "nltk.download('stopwords') # download list of stop words from the nltk corpus\n",
    "from nltk.corpus import stopwords\n",
    "#https://www.w3schools.com/tags/tag_mark.asp (mark tag)\n",
    "def highlight_keywords(text, keywords):\n",
    "    '''\n",
    "        take two arguments, text and keywords in the list\n",
    "        It surrounds keyword with <mark> tags.\n",
    "    '''\n",
    "    for keyword in keywords:\n",
    "        text = re.sub(r'\\b' + keyword + r'\\b', '<mark>' + keyword + '</mark>', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "\n",
    "english_stop_words = stopwords.words(\"english\")\n",
    "french_stop_words = stopwords.words(\"french\")\n",
    "\n",
    "stopwords = english_stop_words + french_stop_words\n",
    "# Create instance of the RAKE algorithm using the combination of english and french stopwords\n",
    "rake = RAKE.Rake(stop_words = stopwords)\n",
    "\n",
    "for i, row in df.iterrows(): #i: index, row: row iterrate over rows in df \n",
    "    keywords = rake.run(row['book_description']) # it returns tuple (keyword, score)\n",
    "    keywords_only = [] # We only need keyword, not score\n",
    "    for keyword in keywords:\n",
    "        keywords_only.append(keyword[0]) # First tuple\n",
    "    highlighted_text = highlight_keywords(row['book_description'], keywords_only)   \n",
    "    # Below code is simpler way (for loop change to list comprehension)    \n",
    "    # highlighted_text = highlight_keywords(row['book_description'], [keyword[0] for keyword in keywords])\n",
    "    df.at[i, 'highlighted_description'] = highlighted_text    \n",
    "# Note: apparently, if you want to access a single cell, df.at is fater than df.loc.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89911d-1d58-4256-9750-90d6853391cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check our output\n",
    "display(HTML(df['highlighted_description'][50]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a764c992-9a45-4ff0-a6c5-4caef800af6b",
   "metadata": {},
   "source": [
    "We can see few book description to see what is their key words. At the moment, most of non stopwords are selected as a keywords. You can try improve this by changing parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffee_coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d429c7232749fcac2bf08c22c3ac5cb4b63ab8bfedcb47b2429d6a145f2521f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
