---
title: "Binary classification with R and Python"
author: "Kayoung Goffe"
date: "04/05/2022"
output: pdf_document
---

```{r setup, include=FALSE}
library(reticulate)
library(mlbench)
library(magrittr)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE)

```

## Reticulate

The {reticulate} package integrates Python with R. 
Today we will run simple machine learning for the binary classification.Binary classification is one of supervised learning method. 


## Python configuration 

We need to create conda environment and install few packages. `conda_list()` function returns an R `data.frame`, with  
name giving the name of the associated environment, and Python giving the path to the Python binary associated with that environment.

```{r check conda list}

conda_list()

```
Why do we create our own conda environment? 

Sometimes you want to have very specific version of Python or related libraries for your project. From my limited experience working on MSc Machine Learning project, I encountered the breaking changes error. For example, Python version was not compatible with tensorflow library.

The environment consists of a certain Python version and relevant packages. It is particularly useful if you want to share your code with other colleagues. (similar reason why some people prefer to use `renv`). Further reading can be found [here:](https://towardsdatascience.com/why-you-should-use-a-virtual-environment-for-every-python-project-c17dab3b0fd0)


So, let's create our own conda environment using `conda_create(cc_env)` 


```{r create my own conda environment}

# define our environment name
cc_env <- "coffee_coding_reticulate"

# create conda environment
conda_create(cc_env)

# check the conda environment
conda_list()
# We can see that new environment has been created.
```
Now we added new environment. Let's install python packages in our coffee and coding environment. 

```{r install python libraries to new environment}

# indicate that we want to use the environment we have just created
use_condaenv("coffee_coding_reticulate")

# install few most commonly used python packages; pandas, numpy, seaborn (for visualisation), scikit-learn (popular machine learning library) 
conda_install("coffee_coding_reticulate", "pandas")
conda_install("coffee_coding_reticulate", "numpy")
conda_install("coffee_coding_reticulate", "seaborn")
conda_install("coffee_coding_reticulate", "matplotlib")
conda_install("coffee_coding_reticulate", "scikit-learn")


```
You can use `py_install()` function as well if you prefer. It will require to install miniconda in your system. We are not using it today as we don't want to install while using binder.

So far we have 1) created our own environment called 'coffee_coding_reticulate' and in that environment, we have installed four Python packages. Let's import these newly installed Python libraries in our R environment.

If you have anaconda installed, you will be able to see the new environment in Anaconda too. You can install Python libraries through Anaconda if you want. I will show you on my local machine as I have installed Anaconda. 

Next, let's read our data using R way.

```{r read diabetes data from mlbench}

data(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)

# Remove NA values and save as diabetes object
diabetes <- na.omit(PimaIndiansDiabetes2)

# splitting data into features and predicted variable
X <- diabetes[,1:8]
# make sure it has binary( 0 = neg, 1=pos) variable
y <- data.frame(diabetes[,9]) %>% 
  dplyr::mutate(outcome = ifelse(`diabetes...9.`=='neg',0,1)) %>% 
  select(outcome)

```

Quick note on each variable:
* Pregnant: Number of times pregnant
* Glucose: Plasma glucose concentration (glucose tolerance test)
* Pressure: Diastolic blood pressure (mm Hg)
* Triceps: Skinfold thickness (mm)
* Insulin: 2-Hr serum insulin (mu U/ml)
* Mass: Body mass index (weight in Kg/ (height in m)Â² )
* Pedigree: Diabetes pedigree function
* Age: Age (years)

### Casting R object to Python using {r_to_py()}

You can try various ways of using R & Python together! I will show two different ways. First, we will **pass R to Python object** using **`{r_to_py()}`**. In this way, we can use Python libraries.


```{r import Python libraries}

# Python essential numpy, pandas are imported
numpy <- import("numpy")
pandas <- import("pandas")
sns <- import("seaborn")
plt <- import("matplotlib.pyplot")

# scikit-learn libraries
# They look different from ususal python way.. 
skl_preprocessing <- import("sklearn.preprocessing")
skl_model_selection <- import("sklearn.model_selection")
skl_linear_model <- import("sklearn.linear_model")
skl_metrics <- import("sklearn.metrics")

```

We now import Python libraries in R. To use these libraries, let's cast R object to Python.

```{r cast R to Python}

py_diabetes <- r_to_py(diabetes)

# check first few rows and try pandas function to see whether it returns correct information.
py_diabetes$head()
py_diabetes$dtypes
py_diabetes$describe() # R summary(diabetes)
# check the length of pandas data frame
py_len(py_diabetes)

```
We can see that we can easily cast R object to Python. Let's do some binary classification using scikit-learn library. We will do following steps:
* Cast X (independent) and y (dependent) to Python object
* Scaling the independent features using `sklearn.preprocessing StandardScaler()` & `fit_transform()`
  * Data standardisation is the process of rescaling the attributes so that they have means as 0 and variance as 1.
  * So it will rescale all the features to a common scale without distorting the differences in the range of the values
  [Further reading](https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe)

```{r split training and test dataset}

py_y = r_to_py(y)
py_X = r_to_py(X)

scaler = skl_preprocessing$StandardScaler()
scaler$fit(py_X)
py_X = scaler$fit_transform(py_X)

# let's quick look whether it did transform the data
# As you can see it now transformed as numpy array
py_X[1:10]
```








 

